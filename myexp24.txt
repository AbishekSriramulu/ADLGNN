Namespace(L1Loss=False, batch_size=12, buildA_true=False, clip=5, conv_channels=16, data='./data/og_dataset.csv', device='cuda:0', dilation_exponential=2, dropout=0.3, end_channels=64, epochs=15, gcn_depth=2, gcn_true=True, horizon=24, in_dim=1, layers=5, log_interval=2000, lr=0.001, node_dim=40, normalize=0, num_nodes=1494, num_split=1, optim='adam', predefinedA_path='./data/og_recon_adj.csv', pretrained_model='./model-RE.pt', propalpha=0.05, residual_channels=16, save='./model-RE24.pt', seq_in_len=168, seq_out_len=1, skip_channels=32, step_size=100, subgraph_size=20, tanhalpha=3, weight_decay=1e-05)
The recpetive field size is 187
Number of model parameters is 472323
Ranger21 optimizer ready with following settings:

Core optimizer = AdamW
Learning rate of 0.001

Important - num_epochs of training = ** 15 epochs **
please confirm this is correct or warmup and warmdown will be off

Warm-up: linear warmup, over 2000 iterations

Lookahead active, merging every 5 steps, with blend factor of 0.5
Stable weight decay of 1e-05
Gradient Centralization = On

Adaptive Gradient Clipping = True
	clipping value of 0.01
	steps for clipping = 0.001

Warm-down: Linear warmdown, starting at 72.0%, iteration 14039 of 19500
warm down will decay until 3e-05 lr
begin training
iter:  0 | loss: 3188.418
iter:100 | loss: 1092.251
iter:200 | loss: 1779.084
iter:300 | loss: 1331.140
iter:400 | loss: 4256.981
iter:500 | loss: 429.208
iter:600 | loss: 670.900
iter:700 | loss: 1934.234
iter:800 | loss: 1044.815
iter:900 | loss: 2707.972
iter:1000 | loss: 1864.512
iter:1100 | loss: 1282.101
iter:1200 | loss: 187.862
| end of epoch   1 | time: 923.31s | train_loss 1049.6385 | valid rse 0.0816 | valid rae 0.0598 | valid corr  0.9407
iter:  0 | loss: 4160.888
iter:100 | loss: 265.709
iter:200 | loss: 254.887
iter:300 | loss: 3465.360
iter:400 | loss: 2932.542
iter:500 | loss: 77.106
iter:600 | loss: 343.107
iter:700 | loss: 2048.772
iter:800 | loss: 1312.402
iter:900 | loss: 5420.535
iter:1000 | loss: 151.823
iter:1100 | loss: 51.294
iter:1200 | loss: 492.682
| end of epoch   2 | time: 923.48s | train_loss 1206.2754 | valid rse 0.0790 | valid rae 0.0540 | valid corr  0.9386
iter:  0 | loss: 4163.082
iter:100 | loss: 328.114
iter:200 | loss: 120.429
iter:300 | loss: 1073.066
iter:400 | loss: 1137.440
iter:500 | loss: 739.398
iter:600 | loss: 949.348
iter:700 | loss: 1244.016
iter:800 | loss: 3717.477
iter:900 | loss: 542.256
iter:1000 | loss: 211.492
iter:1100 | loss: 143.150
iter:1200 | loss: 96.710
| end of epoch   3 | time: 921.92s | train_loss 1935.0093 | valid rse 0.0932 | valid rae 0.0721 | valid corr  0.9354
iter:  0 | loss: 5565.323
iter:100 | loss: 352.458
iter:200 | loss: 510.819
iter:300 | loss: 722.378
iter:400 | loss: 128.989
iter:500 | loss: 73.112
iter:600 | loss: 406.009
iter:700 | loss: 2507.420
iter:800 | loss: 664.864
iter:900 | loss: 1738.000
iter:1000 | loss: 78.723
iter:1100 | loss: 102.327
iter:1200 | loss: 235.584
| end of epoch   4 | time: 922.16s | train_loss 525.8522 | valid rse 0.0795 | valid rae 0.0541 | valid corr  0.9366
iter:  0 | loss: 3743.771
iter:100 | loss: 618.047
iter:200 | loss: 142.500
iter:300 | loss: 1576.805
iter:400 | loss: 3463.336
iter:500 | loss: 1468.582
iter:600 | loss: 322.600
iter:700 | loss: 1960.607
iter:800 | loss: 1497.871
iter:900 | loss: 419.265
iter:1000 | loss: 144.459
iter:1100 | loss: 96.157
iter:1200 | loss: 166.884
| end of epoch   5 | time: 921.94s | train_loss 1759.0989 | valid rse 0.0887 | valid rae 0.0666 | valid corr  0.9375
test rse 0.0788 | test rae 0.0596 | test corr 0.9564
iter:  0 | loss: 5153.179
iter:100 | loss: 238.153
iter:200 | loss: 135.138
iter:300 | loss: 756.688
iter:400 | loss: 224.813
iter:500 | loss: 77.604
iter:600 | loss: 318.640
iter:700 | loss: 2827.670
iter:800 | loss: 786.010
iter:900 | loss: 2609.450
iter:1000 | loss: 273.841
iter:1100 | loss: 344.819
iter:1200 | loss: 172.119
| end of epoch   6 | time: 922.43s | train_loss 627.9586 | valid rse 0.0814 | valid rae 0.0591 | valid corr  0.9337
iter:  0 | loss: 3240.282
iter:100 | loss: 226.541
iter:200 | loss: 2663.459
iter:300 | loss: 1449.972
iter:400 | loss: 91.709
iter:500 | loss: 381.940
iter:600 | loss: 227.306
iter:700 | loss: 2902.366
iter:800 | loss: 771.332
iter:900 | loss: 2869.106
iter:1000 | loss: 93.809
iter:1100 | loss: 88.488
iter:1200 | loss: 746.890
| end of epoch   7 | time: 921.99s | train_loss 1172.1721 | valid rse 0.1061 | valid rae 0.0894 | valid corr  0.9307
iter:  0 | loss: 2510.601
iter:100 | loss: 425.216
iter:200 | loss: 217.938
iter:300 | loss: 770.742
iter:400 | loss: 210.262
iter:500 | loss: 209.821
iter:600 | loss: 1228.334
iter:700 | loss: 10072.480
iter:800 | loss: 5274.907
iter:900 | loss: 2415.846
iter:1000 | loss: 105.485
iter:1100 | loss: 248.756
iter:1200 | loss: 160.361
| end of epoch   8 | time: 921.93s | train_loss 1961.5322 | valid rse 0.0878 | valid rae 0.0653 | valid corr  0.9359
iter:  0 | loss: 4721.324
iter:100 | loss: 243.410
iter:200 | loss: 334.318
iter:300 | loss: 728.720
iter:400 | loss: 169.140
iter:500 | loss: 69.529
iter:600 | loss: 471.712
iter:700 | loss: 2471.488
iter:800 | loss: 687.255
iter:900 | loss: 1966.785
iter:1000 | loss: 131.436
iter:1100 | loss: 126.654
iter:1200 | loss: 239.347
| end of epoch   9 | time: 923.60s | train_loss 546.7081 | valid rse 0.0807 | valid rae 0.0548 | valid corr  0.9344
iter:  0 | loss: 2894.140
iter:100 | loss: 535.473
iter:200 | loss: 97.230
iter:300 | loss: 2260.075
iter:400 | loss: 3961.909
iter:500 | loss: 78.893
iter:600 | loss: 233.071
iter:700 | loss: 2229.961
iter:800 | loss: 1234.062
iter:900 | loss: 3583.642
iter:1000 | loss: 120.260
iter:1100 | loss: 80.438
iter:1200 | loss: 435.163
| end of epoch  10 | time: 922.20s | train_loss 1482.2839 | valid rse 0.0866 | valid rae 0.0633 | valid corr  0.9361
test rse 0.0764 | test rae 0.0559 | test corr 0.9558
iter:  0 | loss: 4039.100
iter:100 | loss: 235.853
iter:200 | loss: 112.014
iter:300 | loss: 919.672
iter:400 | loss: 482.982
iter:500 | loss: 55.104
iter:600 | loss: 299.482
iter:700 | loss: 2585.623
iter:800 | loss: 1818.510
iter:900 | loss: 8095.641
iter:1000 | loss: 408.155
iter:1100 | loss: 55.985
iter:1200 | loss: 521.703
| end of epoch  11 | time: 922.82s | train_loss 1072.0687 | valid rse 0.0832 | valid rae 0.0584 | valid corr  0.9361
iter:  0 | loss: 3595.945
iter:100 | loss: 247.581
iter:200 | loss: 114.004
iter:300 | loss: 1175.812
iter:400 | loss: 861.690
iter:500 | loss: 124.920
iter:600 | loss: 353.706
iter:700 | loss: 2208.959
iter:800 | loss: 2302.193
iter:900 | loss: 8201.634
iter:1000 | loss: 226.340
iter:1100 | loss: 62.977
iter:1200 | loss: 99.909
| end of epoch  12 | time: 923.35s | train_loss 1589.0038 | valid rse 0.0945 | valid rae 0.0734 | valid corr  0.9351
iter:  0 | loss: 4850.778
iter:100 | loss: 254.314
iter:200 | loss: 129.540
iter:300 | loss: 767.258
iter:400 | loss: 242.762
iter:500 | loss: 71.925
iter:600 | loss: 345.617
iter:700 | loss: 2831.240
iter:800 | loss: 800.314
iter:900 | loss: 2663.317
iter:1000 | loss: 307.298
iter:1100 | loss: 350.476
iter:1200 | loss: 170.849
| end of epoch  13 | time: 922.47s | train_loss 629.6922 | valid rse 0.0823 | valid rae 0.0596 | valid corr  0.9328
iter:  0 | loss: 2415.710
iter:100 | loss: 217.199
iter:200 | loss: 3162.866
iter:300 | loss: 1229.450
iter:400 | loss: 81.823
iter:500 | loss: 288.079
iter:600 | loss: 275.230
iter:700 | loss: 2838.970
iter:800 | loss: 803.560
iter:900 | loss: 2987.112
iter:1000 | loss: 81.505
iter:1100 | loss: 64.448
iter:1200 | loss: 741.263
| end of epoch  14 | time: 923.08s | train_loss 1165.7565 | valid rse 0.1080 | valid rae 0.0910 | valid corr  0.9294
iter:  0 | loss: 2262.937
iter:100 | loss: 397.994
iter:200 | loss: 151.313
iter:300 | loss: 789.077
iter:400 | loss: 219.048
iter:500 | loss: 356.936
iter:600 | loss: 1380.492
iter:700 | loss: 9801.751
iter:800 | loss: 4175.371
iter:900 | loss: 3099.472
iter:1000 | loss: 112.890
iter:1100 | loss: 310.990
iter:1200 | loss: 203.006
| end of epoch  15 | time: 923.06s | train_loss 1919.5071 | valid rse 0.0871 | valid rae 0.0635 | valid corr  0.9356
test rse 0.0770 | test rae 0.0560 | test corr 0.9553
final test rse 0.0681 | test rae 0.0455 | test corr 0.9571



Time taken per run



14265.458184727002









10 runs average



valid	rse	rae	corr
mean	0.0790	0.0540	0.9386
std	0.0000	0.0000	0.0000



test	rse	rae	corr
mean	0.0681	0.0455	0.9571
std	0.0000	0.0000	0.0000
