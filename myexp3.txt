Namespace(L1Loss=False, batch_size=12, buildA_true=False, clip=5, conv_channels=16, data='./data/og_dataset.csv', device='cuda:0', dilation_exponential=2, dropout=0.3, end_channels=64, epochs=15, gcn_depth=2, gcn_true=True, horizon=3, in_dim=1, layers=5, log_interval=2000, lr=0.001, node_dim=40, normalize=0, num_nodes=1494, num_split=1, optim='adam', predefinedA_path='./data/og_recon_adj.csv', pretrained_model='./model-RE12.pt', propalpha=0.05, residual_channels=16, save='./model-RE3.pt', seq_in_len=168, seq_out_len=1, skip_channels=32, step_size=100, subgraph_size=20, tanhalpha=3, weight_decay=1e-05)
The recpetive field size is 187
Number of model parameters is 472323
Ranger21 optimizer ready with following settings:

Core optimizer = AdamW
Learning rate of 0.001

Important - num_epochs of training = ** 15 epochs **
please confirm this is correct or warmup and warmdown will be off

Warm-up: linear warmup, over 2000 iterations

Lookahead active, merging every 5 steps, with blend factor of 0.5
Stable weight decay of 1e-05
Gradient Centralization = On

Adaptive Gradient Clipping = True
	clipping value of 0.01
	steps for clipping = 0.001

Warm-down: Linear warmdown, starting at 72.0%, iteration 14061 of 19530
warm down will decay until 3e-05 lr
begin training
iter:  0 | loss: 4777.345
iter:100 | loss: 2185.666
iter:200 | loss: 5995.724
iter:300 | loss: 639.398
iter:400 | loss: 345.933
iter:500 | loss: 308.583
iter:600 | loss: 269.277
iter:700 | loss: 887.565
iter:800 | loss: 556.190
iter:900 | loss: 2348.339
iter:1000 | loss: 795.829
iter:1100 | loss: 2605.425
iter:1200 | loss: 1154.939
iter:1300 | loss: 570.848
| end of epoch   1 | time: 923.76s | train_loss 1257.3267 | valid rse 0.3958 | valid rae 0.4171 | valid corr  0.9652
iter:  0 | loss: 20325.590
iter:100 | loss: 250.683
iter:200 | loss: 518.412
iter:300 | loss: 114.704
iter:400 | loss: 34.351
iter:500 | loss: 40.564
iter:600 | loss: 327.380
iter:700 | loss: 192.036
iter:800 | loss: 224.557
iter:900 | loss: 865.965
iter:1000 | loss: 203.851
iter:1100 | loss: 40.013
iter:1200 | loss: 64.175
iter:1300 | loss: 114.124
| end of epoch   2 | time: 920.56s | train_loss 462.9065 | valid rse 0.5169 | valid rae 0.5472 | valid corr  0.9669
iter:  0 | loss: 25871.943
iter:100 | loss: 4053.307
iter:200 | loss: 48.162
iter:300 | loss: 47.833
iter:400 | loss: 59.665
iter:500 | loss: 46.049
iter:600 | loss: 252.198
iter:700 | loss: 163.369
iter:800 | loss: 156.503
iter:900 | loss: 786.160
iter:1000 | loss: 169.915
iter:1100 | loss: 42.615
iter:1200 | loss: 66.902
iter:1300 | loss: 97.067
| end of epoch   3 | time: 921.39s | train_loss 628.3865 | valid rse 0.0454 | valid rae 0.0322 | valid corr  0.9797
iter:  0 | loss: 8551.173
iter:100 | loss: 355.888
iter:200 | loss: 82.948
iter:300 | loss: 162.750
iter:400 | loss: 382.462
iter:500 | loss: 1169.821
iter:600 | loss: 5235.954
iter:700 | loss: 12929.267
iter:800 | loss: 23507.318
iter:900 | loss: 5049.439
iter:1000 | loss: 1637.556
iter:1100 | loss: 80.503
iter:1200 | loss: 60.778
iter:1300 | loss: 238.308
| end of epoch   4 | time: 921.95s | train_loss 1996.3418 | valid rse 0.3653 | valid rae 0.3846 | valid corr  0.9711
iter:  0 | loss: 16786.527
iter:100 | loss: 353.269
iter:200 | loss: 659.509
iter:300 | loss: 120.234
iter:400 | loss: 33.292
iter:500 | loss: 41.443
iter:600 | loss: 237.472
iter:700 | loss: 144.054
iter:800 | loss: 158.963
iter:900 | loss: 760.287
iter:1000 | loss: 146.586
iter:1100 | loss: 41.628
iter:1200 | loss: 48.700
iter:1300 | loss: 77.571
| end of epoch   5 | time: 923.02s | train_loss 465.7530 | valid rse 0.4851 | valid rae 0.5136 | valid corr  0.9710
test rse 0.4475 | test rae 0.4808 | test corr 0.9773
iter:  0 | loss: 24864.218
iter:100 | loss: 1324.933
iter:200 | loss: 135.408
iter:300 | loss: 39.513
iter:400 | loss: 64.599
iter:500 | loss: 43.296
iter:600 | loss: 202.262
iter:700 | loss: 141.245
iter:800 | loss: 130.778
iter:900 | loss: 752.260
iter:1000 | loss: 139.118
iter:1100 | loss: 43.634
iter:1200 | loss: 51.693
iter:1300 | loss: 73.101
| end of epoch   6 | time: 921.65s | train_loss 770.7925 | valid rse 0.0563 | valid rae 0.0471 | valid corr  0.9815
iter:  0 | loss: 8944.220
iter:100 | loss: 430.590
iter:200 | loss: 71.935
iter:300 | loss: 327.934
iter:400 | loss: 369.015
iter:500 | loss: 380.524
iter:600 | loss: 1459.009
iter:700 | loss: 1131.559
iter:800 | loss: 3356.809
iter:900 | loss: 17707.063
iter:1000 | loss: 766.520
iter:1100 | loss: 430.114
iter:1200 | loss: 368.700
iter:1300 | loss: 129.422
| end of epoch   7 | time: 923.87s | train_loss 1342.2584 | valid rse 0.4119 | valid rae 0.4359 | valid corr  0.9728
iter:  0 | loss: 18598.219
iter:100 | loss: 742.663
iter:200 | loss: 206.712
iter:300 | loss: 52.113
iter:400 | loss: 48.941
iter:500 | loss: 39.221
iter:600 | loss: 200.229
iter:700 | loss: 122.947
iter:800 | loss: 136.326
iter:900 | loss: 775.879
iter:1000 | loss: 135.165
iter:1100 | loss: 44.836
iter:1200 | loss: 47.956
iter:1300 | loss: 69.452
| end of epoch   8 | time: 921.90s | train_loss 617.9430 | valid rse 0.2599 | valid rae 0.2723 | valid corr  0.9766
iter:  0 | loss: 12407.817
iter:100 | loss: 268.779
iter:200 | loss: 1007.846
iter:300 | loss: 114.928
iter:400 | loss: 33.702
iter:500 | loss: 45.455
iter:600 | loss: 181.052
iter:700 | loss: 309.812
iter:800 | loss: 140.072
iter:900 | loss: 720.353
iter:1000 | loss: 196.509
iter:1100 | loss: 2088.822
iter:1200 | loss: 5060.024
iter:1300 | loss: 1275.423
| end of epoch   9 | time: 922.92s | train_loss 1208.7461 | valid rse 0.3283 | valid rae 0.3295 | valid corr  0.8803
iter:  0 | loss: 18422.847
iter:100 | loss: 6683.764
iter:200 | loss: 5586.404
iter:300 | loss: 1987.895
iter:400 | loss: 1125.535
iter:500 | loss: 1705.862
iter:600 | loss: 1208.307
iter:700 | loss: 1153.930
iter:800 | loss: 1246.521
iter:900 | loss: 1305.116
iter:1000 | loss: 547.067
iter:1100 | loss: 387.601
iter:1200 | loss: 583.849
iter:1300 | loss: 474.268
| end of epoch  10 | time: 923.07s | train_loss 4745.5176 | valid rse 0.6406 | valid rae 0.6855 | valid corr  0.9666
test rse 0.5920 | test rae 0.6429 | test corr 0.9743
iter:  0 | loss: 33785.321
iter:100 | loss: 1085.612
iter:200 | loss: 761.256
iter:300 | loss: 300.332
iter:400 | loss: 207.898
iter:500 | loss: 236.457
iter:600 | loss: 326.821
iter:700 | loss: 268.384
iter:800 | loss: 266.342
iter:900 | loss: 794.205
iter:1000 | loss: 106.676
iter:1100 | loss: 86.351
iter:1200 | loss: 150.957
iter:1300 | loss: 112.432
| end of epoch  11 | time: 923.81s | train_loss 2255.8252 | valid rse 0.2174 | valid rae 0.2290 | valid corr  0.9799
iter:  0 | loss: 11560.808
iter:100 | loss: 325.067
iter:200 | loss: 267.405
iter:300 | loss: 73.520
iter:400 | loss: 58.451
iter:500 | loss: 59.714
iter:600 | loss: 215.779
iter:700 | loss: 139.059
iter:800 | loss: 157.075
iter:900 | loss: 760.046
iter:1000 | loss: 64.487
iter:1100 | loss: 40.026
iter:1200 | loss: 64.196
iter:1300 | loss: 84.118
| end of epoch  12 | time: 922.20s | train_loss 286.4405 | valid rse 0.0790 | valid rae 0.0769 | valid corr  0.9822
iter:  0 | loss: 10308.619
iter:100 | loss: 444.649
iter:200 | loss: 108.976
iter:300 | loss: 46.337
iter:400 | loss: 38.066
iter:500 | loss: 47.679
iter:600 | loss: 232.321
iter:700 | loss: 142.438
iter:800 | loss: 155.982
iter:900 | loss: 806.277
iter:1000 | loss: 137.526
iter:1100 | loss: 38.005
iter:1200 | loss: 54.441
iter:1300 | loss: 71.174
| end of epoch  13 | time: 921.85s | train_loss 134.0644 | valid rse 0.1670 | valid rae 0.1737 | valid corr  0.9811
iter:  0 | loss: 10724.295
iter:100 | loss: 280.352
iter:200 | loss: 42.585
iter:300 | loss: 1033.309
iter:400 | loss: 1759.221
iter:500 | loss: 1707.721
iter:600 | loss: 2148.426
iter:700 | loss: 1691.159
iter:800 | loss: 3154.343
iter:900 | loss: 15975.632
iter:1000 | loss: 1442.274
iter:1100 | loss: 449.098
iter:1200 | loss: 170.015
iter:1300 | loss: 83.090
| end of epoch  14 | time: 922.63s | train_loss 1257.6187 | valid rse 0.3083 | valid rae 0.3265 | valid corr  0.9769
iter:  0 | loss: 14678.676
iter:100 | loss: 1484.297
iter:200 | loss: 38.228
iter:300 | loss: 41.078
iter:400 | loss: 45.581
iter:500 | loss: 39.535
iter:600 | loss: 211.082
iter:700 | loss: 128.820
iter:800 | loss: 160.862
iter:900 | loss: 806.732
iter:1000 | loss: 93.761
iter:1100 | loss: 35.820
iter:1200 | loss: 39.521
iter:1300 | loss: 68.336
| end of epoch  15 | time: 923.05s | train_loss 330.4599 | valid rse 0.1335 | valid rae 0.1358 | valid corr  0.9811
test rse 0.1239 | test rae 0.1281 | test corr 0.9856
final test rse 0.0386 | test rae 0.0268 | test corr 0.9850



Time taken per run



14262.429226587003









10 runs average



valid	rse	rae	corr
mean	0.0454	0.0322	0.9797
std	0.0000	0.0000	0.0000



test	rse	rae	corr
mean	0.0386	0.0268	0.9850
std	0.0000	0.0000	0.0000
